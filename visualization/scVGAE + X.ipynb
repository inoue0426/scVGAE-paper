{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "structured-hypothetical",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.resetwarnings()\n",
    "\n",
    "import scprep\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch_geometric\n",
    "from torch.nn.functional import relu, softplus\n",
    "from torch.nn import Linear, Module, Dropout, MSELoss, CrossEntropyLoss, BatchNorm1d\n",
    "\n",
    "from torch_geometric.nn import GCNConv, GATConv, GraphNorm\n",
    "from torch_geometric.data import Data\n",
    "from torch_sparse import SparseTensor\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score, normalized_mutual_info_score\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import optuna\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'\n",
    "device = 1\n",
    "device = torch.device(\"cuda:{}\".format(device) if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "biblical-transition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topX(X):\n",
    "    return X * np.array(X > np.percentile(X, 85), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pressing-victim",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj(x):\n",
    "    adj = SparseTensor(\n",
    "        row= torch.tensor(np.array(x.nonzero()))[0], \n",
    "        col= torch.tensor(np.array(x.nonzero()))[1], \n",
    "        sparse_sizes=(x.shape[0], x.shape[0])\n",
    "    ).to(device)\n",
    "    return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "impaired-broad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(X, metric='linear'):\n",
    "    dist = pairwise_kernels(X, metric=metric)\n",
    "    dist_x = get_topX(dist)\n",
    "    return torch.tensor(X.values, dtype=torch.float).to(device), get_adj(dist_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "turned-tackle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_i(i):\n",
    "    df = pd.read_csv('../data/{}/data.csv.gz'.format(i), index_col=0)\n",
    "    tmp = np.sign(df)\n",
    "    cols = (np.sum(tmp) > int((df.shape[0])*0.05))\n",
    "    rows = (np.sum(tmp, axis=1) > int((df.shape[1])*0.05))\n",
    "    df = np.log(df.loc[rows, cols] + 1)\n",
    "    df_norm = df.copy()\n",
    "    df_norm = scprep.normalize.library_size_normalize(df_norm)    \n",
    "    df_norm = scprep.transform.sqrt(df_norm)\n",
    "    X_norm = pd.DataFrame(df_norm, columns=df.columns)\n",
    "    labels = df.index\n",
    "    data = torch.tensor(df_norm.values, dtype=torch.float).to(device)\n",
    "    return df_norm, labels, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-integral",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "offensive-monte",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ZINBLoss(y_true, y_pred, theta, pi, eps=1e-10):\n",
    "    \"\"\"\n",
    "    Compute the ZINB Loss.\n",
    "    \n",
    "    y_true: Ground truth data.\n",
    "    y_pred: Predicted mean from the model.\n",
    "    theta: Dispersion parameter.\n",
    "    pi: Zero-inflation probability.\n",
    "    eps: Small constant to prevent log(0).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Negative Binomial Loss\n",
    "    nb_terms = -torch.lgamma(y_true + theta) + torch.lgamma(y_true + 1) + torch.lgamma(theta) \\\n",
    "               - theta * torch.log(theta + eps) \\\n",
    "               + theta * torch.log(theta + y_pred + eps) \\\n",
    "               - y_true * torch.log(y_pred + theta + eps) \\\n",
    "               + y_true * torch.log(y_pred + eps)\n",
    "    \n",
    "    # Zero-Inflation\n",
    "    zero_inflated = torch.log(pi + (1 - pi) * torch.pow(1 + y_pred / theta, -theta))\n",
    "    \n",
    "    result = -torch.sum(torch.log(pi + (1 - pi) * torch.pow(1 + y_pred / theta, -theta)) * (y_true < eps).float() \\\n",
    "                        + (1 - (y_true < eps).float()) * nb_terms)\n",
    "    \n",
    "    return torch.round(result, decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "weighted-temperature",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(x_original, x_recon, z_mean, z_dropout, z_dispersion, alpha):\n",
    "    \"\"\"\n",
    "    Compute the combined loss: ZINB Loss + MSE Loss.\n",
    "    \n",
    "    Parameters:\n",
    "    - x_original: Original data matrix.\n",
    "    - x_recon: Reconstructed matrix from the model.\n",
    "    - z_mean, z_dropout, z_dispersion: Outputs from the model, used for ZINB Loss calculation.\n",
    "    - device: Device to which tensors should be moved before computation.\n",
    "    - lambda_1, lambda_2: Weights for ZINB Loss and MSE Loss respectively.\n",
    "    \n",
    "    Returns:\n",
    "    - total_loss: Combined loss value.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute ZINB Loss (assuming ZINBLoss is a properly defined function or class)\n",
    "    zinb_loss = ZINBLoss(x_original, z_mean, z_dispersion, z_dropout)\n",
    "    \n",
    "    # Compute MSE Loss\n",
    "    mse_loss = MSELoss()(x_recon, x_original)\n",
    "    \n",
    "    # Combine the losses\n",
    "    total_loss = alpha * zinb_loss + (1-alpha) * mse_loss\n",
    "    \n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "conditional-australian",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGAE(Module):\n",
    "    def __init__(\n",
    "        self, input_dim, hidden0, hidden1, hidden2, \n",
    "        # hidden3, \n",
    "        dropout1, dropout2, \n",
    "        # dropout4\n",
    "    ):\n",
    "        super(VGAE, self).__init__()\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(dropout1)\n",
    "        self.dropout2 = nn.Dropout(dropout2)\n",
    "        # self.dropout4 = nn.Dropout(dropout4)\n",
    "        \n",
    "        # Encoder with 2 gat layers\n",
    "        self.gat1 = GCNConv(input_dim, hidden1)\n",
    "        self.gn1 = GraphNorm(hidden1)  # Batch normalization after first gat layer\n",
    "        self.gat2_mean = GCNConv(hidden1, input_dim)\n",
    "        self.gat2_dropout = GCNConv(hidden1, input_dim)\n",
    "        self.gat2_dispersion = GCNConv(hidden1, input_dim)\n",
    "\n",
    "        # Decoder with 2 Linear layers\n",
    "        self.fc1 = Linear(input_dim, hidden2)\n",
    "        self.bn2 = BatchNorm1d(hidden2)  # Batch normalization after first linear layer\n",
    "        self.fc2 = Linear(hidden2, input_dim)\n",
    "        \n",
    "        self.batch_norm1 = BatchNorm1d(input_dim)\n",
    "        self.batch_norm2 = BatchNorm1d(hidden0)\n",
    "        \n",
    "    def encode(self, x, adj):\n",
    "        x = relu(self.gn1(self.gat1(x, adj)))  # Apply ReLU and GraphNorm\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        z_mean = torch.exp(self.gat2_mean(x, adj.t()))\n",
    "        z_dropout = torch.sigmoid(self.gat2_dropout(x, adj.t()))\n",
    "        z_dispersion = torch.exp(self.gat2_dispersion(x, adj.t()))\n",
    "        return z_mean, z_dropout, z_dispersion\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = relu(self.bn2(self.fc1(z)))  # Apply ReLU and BatchNorm\n",
    "        z = self.dropout2(z)\n",
    "        return relu(self.fc2(z))\n",
    "\n",
    "    def forward(self, x, adj, x_t, adj_t, ):\n",
    "        z_mean, z_dropout, z_dispersion = self.encode(x, adj.t())\n",
    "        x_recon = self.decode(z_mean) + self.batch_norm1(x) + self.batch_norm2(x_t).T\n",
    "        return x_recon, z_mean, z_dropout, z_dispersion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "minimal-swift",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:38<00:00,  2.57it/s]\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "\n",
    "alpha=0.05\n",
    "dropout1=0.2\n",
    "dropout2=0.4\n",
    "epochs=100\n",
    "hidden1=128\n",
    "hidden2=1024\n",
    "lr=0.0001\n",
    "\n",
    "df_norm, labels, data = get_data_for_i('brosens')\n",
    "x, adj = get_data(df_norm)\n",
    "x_t, adj_t = get_data(df_norm.T)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "input_dim = df_norm.shape[1]\n",
    "hidden0 = df_norm.shape[0]\n",
    "\n",
    "model = VGAE(input_dim, hidden0, hidden1, hidden2, \n",
    "             dropout1, dropout2, \n",
    "#                  dropout4\n",
    "            ).to(device)\n",
    "optimizer_name = 'Adam'\n",
    "optimizer = getattr(torch.optim, optimizer_name)(\n",
    "    model.parameters(), \n",
    "    lr=lr, \n",
    ")\n",
    "\n",
    "losses = []\n",
    "for epoch in tqdm(range(epochs)): \n",
    "    # Forward pass\n",
    "    x_recon, z_mean, z_dropout, z_dispersion = model(x, adj, x_t, adj_t)\n",
    "\n",
    "    # Compute the ZINB Loss using the outputs from the model\n",
    "    loss = compute_loss(x, x_recon, z_mean, z_dispersion, z_dropout, alpha).to(device)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step() \n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "pred = x_recon.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "quick-professor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.9266362 , -0.8055201 ,  3.1580687 , ..., -1.2080903 ,\n",
       "         1.3530047 , -0.46975672],\n",
       "       [ 1.2539519 , -0.43764263, -0.49765468, ...,  3.139907  ,\n",
       "         1.2629893 , -0.8915373 ],\n",
       "       [ 2.9251595 , -0.66047966, -1.0427344 , ..., -1.1094288 ,\n",
       "         2.0136003 ,  2.9641027 ],\n",
       "       ...,\n",
       "       [ 0.05846319, -0.6997969 , -0.4236164 , ...,  6.136872  ,\n",
       "         0.03928235, -0.17400971],\n",
       "       [-0.24071535, -0.46588382, -0.0536544 , ..., -0.12295473,\n",
       "        -0.43186972, -0.1757173 ],\n",
       "       [ 5.253864  , -0.03770646, -0.07805464, ...,  6.4354267 ,\n",
       "        -0.6020671 , -0.03226513]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "absolute-triple",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pred).to_csv('result/scVGAE.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-excuse",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-jersey",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-aviation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-direction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-bookmark",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-combination",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-burlington",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-cliff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-respect",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-diana",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-platinum",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-tomorrow",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu1",
   "language": "python",
   "name": "gpu1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
